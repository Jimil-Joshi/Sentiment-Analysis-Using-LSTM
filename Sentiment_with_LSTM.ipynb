{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXOikU5aiLxJ",
        "outputId": "d8c173bc-69b4-4284-92b3-76446debd0fa"
      },
      "id": "VXOikU5aiLxJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c08655e-e1fb-48bf-a6e7-649e3c5c3cd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c08655e-e1fb-48bf-a6e7-649e3c5c3cd5",
        "outputId": "ff71d69c-f4bd-456f-b5c9-0a2e30a29132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "0_wBZQEvDptS"
      },
      "id": "0_wBZQEvDptS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(train_data):\n",
        "    df = pd.read_csv(train_data)\n",
        "    # df2 = pd.read_csv(\"/content/drive/MyDrive/Senti_Task/CSV/Test/merged_test.csv\")\n",
        "    x_data = df['comment']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    english_stops = set(stopwords.words('english'))\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    # x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops ])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_train, y_train  = load_dataset(\"/content/drive/MyDrive/Senti_Task/CSV/Train/merged_train.csv\")\n",
        "\n",
        "print('Reviews')\n",
        "print(x_train, '\\n')\n",
        "print('Sentiment')\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lo176L3tsEP",
        "outputId": "aeeb36c5-6fb9-4fa5-e86b-226a9473567c"
      },
      "id": "8Lo176L3tsEP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews\n",
            "0        [story, man, unnatural, feelings, pig, starts,...\n",
            "1        [airport, 77, starts, brand, new, luxury, 747,...\n",
            "2        [film, lacked, something, couldnt, put, finger...\n",
            "3        [sorry, everyone, know, supposed, art, film, w...\n",
            "4        [little, parents, took, along, theater, see, i...\n",
            "                               ...                        \n",
            "24995    [seeing, vote, average, pretty, low, fact, cle...\n",
            "24996    [plot, wretched, unbelievable, twists, however...\n",
            "24997    [amazed, movieand, others, average, 5, stars, ...\n",
            "24998    [christmas, together, actually, came, time, iv...\n",
            "24999    [workingclass, romantic, drama, director, mart...\n",
            "Name: comment, Length: 25000, dtype: object \n",
            "\n",
            "Sentiment\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "24995    1\n",
            "24996    1\n",
            "24997    1\n",
            "24998    1\n",
            "24999    1\n",
            "Name: sentiment, Length: 25000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "oov_tok = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "print(\"Number of Documents: \", tokenizer.document_count)\n",
        "print(\"Number of Words: \", tokenizer.num_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnOutg2rCbg5",
        "outputId": "1eec8c0e-1d9d-471d-94c0-d21563fa7701"
      },
      "id": "lnOutg2rCbg5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents:  25000\n",
            "Number of Words:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "print(train_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIuvw-KxCn_m",
        "outputId": "3bd234f7-3e78-49ab-8476-71886b15f65c"
      },
      "id": "wIuvw-KxCn_m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 50, 7492, 1247, 4500, 390, 503, 51, 1157, 346, 1631, 123, 1, 7611, 206, 564, 1964, 978, 2885, 801, 1, 5281, 350, 2467, 1631, 122, 10, 689, 1175, 710, 147, 1346, 8, 951, 564, 1, 301, 9, 25, 2112, 187, 657, 724, 1, 1549, 567, 47, 130, 30, 7, 496, 594, 20, 1, 1, 594, 285, 3344, 1, 1, 8263, 34, 3189]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 200\n",
        "train_padded = pad_sequences(train_sequences, maxlen=sequence_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "3Ug_36_mCuFf"
      },
      "id": "3Ug_36_mCuFf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test  = load_dataset(\"/content/drive/MyDrive/Senti_Task/CSV/Test/merged_test.csv\")\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=sequence_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "4PolcPrpDdVk"
      },
      "id": "4PolcPrpDdVk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 32\n",
        "lstm_out = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Added dropout for regularization\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVDN4ZmhC4TJ",
        "outputId": "8acf58a8-c9ae-495e-a5a0-7bdb4498b73b"
      },
      "id": "qVDN4ZmhC4TJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 32)           320000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               49664     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 374133 (1.43 MB)\n",
            "Trainable params: 374133 (1.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = os.getcwd()\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)\n",
        "callbacks = [EarlyStopping(patience=2), model_checkpoint_callback]"
      ],
      "metadata": {
        "id": "UUAqmS72DIsv"
      },
      "id": "UUAqmS72DIsv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_padded, y_train, epochs=10, validation_data=(test_padded, y_test), callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTkV5SsDDLUs",
        "outputId": "db3c592d-883f-4fed-ee1d-4672106f91f2"
      },
      "id": "PTkV5SsDDLUs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 1282s 2s/step - loss: 0.4355 - accuracy: 0.7954 - val_loss: 0.3528 - val_accuracy: 0.8474\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 1192s 2s/step - loss: 0.2638 - accuracy: 0.9052 - val_loss: 0.5115 - val_accuracy: 0.8261\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 1163s 1s/step - loss: 0.2010 - accuracy: 0.9281 - val_loss: 0.3415 - val_accuracy: 0.8515\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 1182s 2s/step - loss: 0.1626 - accuracy: 0.9433 - val_loss: 0.5702 - val_accuracy: 0.8098\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 1187s 2s/step - loss: 0.1371 - accuracy: 0.9528 - val_loss: 0.4223 - val_accuracy: 0.8296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(history.history)\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyQO1-6qDOFf",
        "outputId": "2d787e88-9901-4b17-f5ad-be7608e9844e"
      },
      "id": "SyQO1-6qDOFf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       loss  accuracy  val_loss  val_accuracy\n",
            "0  0.435463   0.79540  0.352795      0.847355\n",
            "1  0.263754   0.90516  0.511506      0.826137\n",
            "2  0.201042   0.92812  0.341468      0.851473\n",
            "3  0.162614   0.94332  0.570158      0.809837\n",
            "4  0.137098   0.95284  0.422329      0.829568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided data, it appears to be a summary of the performance of a machine learning model across five training epochs. Let's analyze the key metrics:\n",
        "\n",
        "1. **Loss:**\n",
        "   - Initial epoch: 0.435463\n",
        "   - Second epoch: 0.263754\n",
        "   - Third epoch: 0.201042\n",
        "   - Fourth epoch: 0.162614\n",
        "   - Fifth epoch: 0.137098\n",
        "\n",
        "   The decreasing trend in training loss over epochs indicates that the model is learning and improving its performance on the training data.\n",
        "\n",
        "2. **Accuracy:**\n",
        "   - Initial epoch: 0.79540 (79.54%)\n",
        "   - Second epoch: 0.90516 (90.52%)\n",
        "   - Third epoch: 0.92812 (92.81%)\n",
        "   - Fourth epoch: 0.94332 (94.33%)\n",
        "   - Fifth epoch: 0.95284 (95.28%)\n",
        "\n",
        "   The increasing trend in training accuracy suggests that the model is getting better at correctly classifying instances in the training set.\n",
        "\n",
        "3. **Validation Loss and Accuracy:**\n",
        "   - Initial epoch: val_loss = 0.352795, val_accuracy = 0.847355 (84.74%)\n",
        "   - Second epoch: val_loss = 0.511506, val_accuracy = 0.826137 (82.61%)\n",
        "   - Third epoch: val_loss = 0.341468, val_accuracy = 0.851473 (85.15%)\n",
        "   - Fourth epoch: val_loss = 0.570158, val_accuracy = 0.809837 (80.98%)\n",
        "   - Fifth epoch: val_loss = 0.422329, val_accuracy = 0.829568 (82.96%)\n",
        "\n",
        "   The validation metrics provide insight into how well the model generalizes to unseen data. The fluctuation in validation metrics, especially the increase in validation loss in the fourth epoch, suggests some level of overfitting or instability.\n",
        "\n",
        "In summary, the model shows improvement in training accuracy and loss over the five epochs. However, there are signs of potential overfitting or instability, as indicated by the fluctuating validation metrics. Further analysis and potential adjustments, such as regularization techniques or model architecture changes, might be necessary to enhance generalization performance. Monitoring these metrics in future training epochs and iterations is crucial for refining the model."
      ],
      "metadata": {
        "id": "jyyCYwYyVpMU"
      },
      "id": "jyyCYwYyVpMU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad the testing data\n",
        "X_test_sequence = tokenizer.texts_to_sequences(x_test)\n",
        "X_test_padded = pad_sequences(X_test_sequence, maxlen=sequence_length)\n",
        "\n",
        "# Load the trained model\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test_padded)\n",
        "\n",
        "# Convert predictions to binary labels (0 or 1) based on a threshold (e.g., 0.5)\n",
        "threshold = 0.7\n",
        "binary_predictions = (predictions > threshold).astype(int)\n",
        "\n",
        "# Display the predictions\n",
        "print(binary_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9JyIxlZYJI4",
        "outputId": "6bb4f802-ed92-45b4-a2f6-d445c7888dfd"
      },
      "id": "N9JyIxlZYJI4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 60/547 [==>...........................] - ETA: 53s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "true = 0\n",
        "for i, y in enumerate(y_test):\n",
        "    if y == binary_predictions[i]:\n",
        "        true += 1\n",
        "\n",
        "print('Correct Prediction: {}'.format(true))\n",
        "print('Wrong Prediction: {}'.format(len(binary_predictions) - true))\n",
        "print('Accuracy: {}'.format(true/len(binary_predictions)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6tP1qYPuKpI",
        "outputId": "8b268e09-a624-402c-b557-14ac2bb511ce"
      },
      "id": "W6tP1qYPuKpI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct Prediction: 13516\n",
            "Wrong Prediction: 3969\n",
            "Accuracy: 77.30054332284816\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}